{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.61.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#随机更换代理，是反爬的trick之一，但是在实践过程中，并不work；\n",
    "#更换header也是，但是从爬取的实践经验来看，time.sleep()的设置是非常重要的；\n",
    "#sleep的时间设置长些，也就意味着访问url的间隔时间会长些，这样可以有效避免反爬；\n",
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "import sys\n",
    "!{sys.executable} -m pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "import csv\n",
    "from itertools import cycle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "!{sys.executable} -m pip install tqdm\n",
    "import tqdm\n",
    "\n",
    "def get_data(page, HEADER):\n",
    "    #一页有30个帖子\n",
    "    basic_url = 'https://www.douban.com/group/634189/discussion?start=%d'%(page*30)\n",
    "    #basic_url = 'https://www.douban.com/group/638298/discussion?start=0'\n",
    "    response = requests.get(basic_url, headers = HEADER, timeout= 60)\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    #获取帖子中的title,name,response,time;\n",
    "    data = soup.find_all('tr')\n",
    "    #filtered_data[i] = ['精品', 'title','name','response_nums','time'] or ['title','name','response_nums','time']\n",
    "    filtered_data = [[i for i in i.text.strip().split('\\n')if i.strip() != ''] for i in data[1:]]\n",
    "    \n",
    "    #获取帖子中的topic's url及发帖人id；\n",
    "    topic_people_urls = [url.get('href') for url in soup.find_all('a')]\n",
    "    topic_url = []\n",
    "    people_url = []\n",
    "    for topic_people_url in topic_people_urls:\n",
    "        if topic_people_url is not None:\n",
    "            topic_people_url_ = topic_people_url.split('/')\n",
    "            #topic_url in one page;\n",
    "            if len(topic_people_url_) < 3:\n",
    "                continue\n",
    "            if topic_people_url_[-3] == 'topic':\n",
    "                topic_url.append(topic_people_url)\n",
    "            #people_url in one page;\n",
    "            elif topic_people_url_[-3] == 'people':\n",
    "                people_url.append(topic_people_url)\n",
    "    \n",
    "             \n",
    "    return filtered_data, topic_url, people_url\n",
    "\n",
    "def get_most_popular_cont(soup):\n",
    "    dataOfMostPopular = soup.find_all('ul', \"topic-reply popular-bd\")[0]\n",
    "\n",
    "    #最赞回应的发帖方式（电脑orApp）；\n",
    "    waysToPostsInMostPopular = []\n",
    "    for i in dataOfMostPopular.find_all('li'):\n",
    "        try:\n",
    "            wayToPost = i.find_all('span', 'via')[0].text\n",
    "        except:\n",
    "            wayToPost = ''\n",
    "        waysToPostsInMostPopular.append(wayToPost)\n",
    "\n",
    "    #最赞回应的发帖内容；\n",
    "    contsOfMostPopular = [i.text for i in dataOfMostPopular.find_all('p')]\n",
    "\n",
    "    #最赞回应用户的nickname；\n",
    "    nickNamesOfMostPopular = [i.text.split('\\n')[1] for i in dataOfMostPopular.find_all('h4')]\n",
    "\n",
    "    #最赞回应用户的id；\n",
    "    idsOfMostPopular = [i.find('a').get('href') for i in dataOfMostPopular.find_all('h4')]\n",
    "    \n",
    "    return waysToPostsInMostPopular, contsOfMostPopular, nickNamesOfMostPopular, idsOfMostPopular\n",
    "\n",
    "\n",
    "def get_way_id_name_cont(soup):\n",
    "    data = soup.find_all('div', 'reply-doc content')\n",
    "    #发帖方式（电脑orApp）\n",
    "    ways = []\n",
    "    for i in data:\n",
    "        try:\n",
    "            way = i.find_all('span', 'via')[0].text\n",
    "        except:\n",
    "            way = ''\n",
    "        ways.append(way)\n",
    "\n",
    "    #回复内容；\n",
    "    conts = [j.text for i in data for j in i.find_all('p')]\n",
    "\n",
    "    #回复人的nickname;\n",
    "    nickNames = [i.find_all('a')[0].text for i in data]\n",
    "    \n",
    "    #回复人的id\n",
    "    ids = [i.find_all('a')[0].get('href') for i in data]\n",
    "    \n",
    "    return ways, conts, nickNames, ids\n",
    "\n",
    "\n",
    "def get_content(topic_url, custom):\n",
    "    \n",
    "    allWaysOfReplyContents = []\n",
    "    allContsOfReplyContents = []\n",
    "    allNicknamesOfReplyContents = []\n",
    "    allIdsOfReplyContents = []\n",
    "    allWaysOfReplyContents_ = []\n",
    "    allContsOfReplyContents_ = []\n",
    "    allNicknamesOfReplyContents_ = []\n",
    "    allIdsOfReplyContents_ = []\n",
    "    imgNumsInTopic_ = []\n",
    "    topicDocs_ = []\n",
    "    topicWays_ = []\n",
    "    most_popular = []\n",
    "    \n",
    "    for url in topic_url:\n",
    "        HEADER = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "        response = requests.get(url, headers=HEADER, timeout= 60)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "        #帖子中包含图片的数量；\n",
    "        try:\n",
    "            imgNumsInTopic = len(soup.find_all('div', \"rich-content topic-richtext\")[0].find_all('img'))\n",
    "        except:\n",
    "            imgNumsInTopic = 0\n",
    "        imgNumsInTopic_.append(imgNumsInTopic)\n",
    "        \n",
    "        topic = [i for i in soup.find_all('div','topic-doc')[0].text.split('\\n') if (i != '' and len(soup.find_all('div','topic-doc')) > 0)]\n",
    "        \n",
    "        try:\n",
    "            topicDocs = ''.join([i.text for i in soup.find_all('div', \"rich-content topic-richtext\")[0].find_all('p')])\n",
    "        except:\n",
    "            topicDocs = '出现list_index错误'\n",
    "        topicWays = topic[-1]\n",
    "        \n",
    "        topicDocs_.append(topicDocs)\n",
    "        topicWays_.append(topicWays)\n",
    "        \n",
    "        if custom == 'mostPopular':\n",
    "            #最赞回应的发帖方式（电脑orApp）、内容、昵称及id；\n",
    "            waysToPostsInMostPopular, contsOfMostPopular, nickNamesOfMostPopular, idsOfMostPopular = get_most_popular_cont(soup)\n",
    "            allWaysOfReplyContents_.append(waysToPostsInMostPopular)\n",
    "            allContsOfReplyContents_.append(contsOfMostPopular)\n",
    "            allNicknamesOfReplyContents_.append(nickNamesOfMostPopular)\n",
    "            allIdsOfReplyContents_.append(idsOfMostPopular)\n",
    "        \n",
    "        elif custom == '1stPage':\n",
    "            #第一页的发帖方式（电脑orApp）、内容、昵称及id；\n",
    "            waysOfReplyIn1stPage, contsOfReplyIn1stPage, nickNamesOfReplyIn1stPage, idsOfReplyIn1stPage = get_way_id_name_cont(soup)\n",
    "            try:\n",
    "                len_popular, _, _, _ = get_most_popular_cont(soup)\n",
    "                index = len(len_popular)\n",
    "            except:\n",
    "                index = 'none'\n",
    "            allWaysOfReplyContents_.append(waysOfReplyIn1stPage)\n",
    "            allContsOfReplyContents_.append(contsOfReplyIn1stPage)\n",
    "            allNicknamesOfReplyContents_.append(nickNamesOfReplyIn1stPage)\n",
    "            allIdsOfReplyContents_.append(idsOfReplyIn1stPage)\n",
    "            most_popular.append(index)\n",
    "            \n",
    "        elif custom == 'allPages':\n",
    "            #全部页的发帖方式（电脑orApp）、内容、昵称及id；\n",
    "#             waysOfReplyIn1stPage, contsOfReplyIn1stPage, nickNamesOfReplyIn1stPage, idsOfReplyIn1stPage = get_way_id_name_cont(soup)\n",
    "#             allWaysOfReplyContents.extend(waysOfReplyIn1stPage)\n",
    "#             allContsOfReplyContents.extend(contsOfReplyIn1stPage)\n",
    "#             allNicknamesOfReplyContents.extend(nickNamesOfReplyIn1stPage)\n",
    "#             allIdsOfReplyContents.extend(idsOfReplyIn1stPage)\n",
    "            # TODO: 写一个for loop取所有page的url\n",
    "            allPagesOfReplyUrls = [url]\n",
    "            getNextReplyPageUrl(url, allPagesOfReplyUrls)\n",
    "            for page in allPagesOfReplyUrls:\n",
    "                response = requests.get(page, headers = HEADER, timeout= 60)\n",
    "                htmlOfPage = response.text\n",
    "                soupOfPage = BeautifulSoup(html, 'lxml')\n",
    "                ways, conts, nickNames, ids = get_way_id_name_cont(soupOfPage)\n",
    "                allWaysOfReplyContents.extend(ways)\n",
    "                allContsOfReplyContents.extend(conts)\n",
    "                allNicknamesOfReplyContents.extend(nickNames)\n",
    "                allIdsOfReplyContents.extend(ids)\n",
    "                time.sleep(3)\n",
    "            \n",
    "            allWaysOfReplyContents_.append(allWaysOfReplyContents)\n",
    "            allContsOfReplyContents_.append(allContsOfReplyContents)\n",
    "            allNicknamesOfReplyContents_.append(allNicknamesOfReplyContents)\n",
    "            allIdsOfReplyContents_.append(allIdsOfReplyContents)\n",
    "            \n",
    "        time.sleep(3)\n",
    "            \n",
    "    return imgNumsInTopic_, topicDocs_, topicWays_, allWaysOfReplyContents_, allContsOfReplyContents_, allNicknamesOfReplyContents_, allIdsOfReplyContents_, most_popular\n",
    "\n",
    "def getNextReplyPageUrl(url, res):\n",
    "    '''\n",
    "\n",
    "    :return:\n",
    "    '''\n",
    "    if url==None or len(url)==0:\n",
    "        return\n",
    "    try:\n",
    "        response = requests.get(url, headers = HEADER, timeout= 60)\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        data = soup.findAll('span', 'next')\n",
    "        # 帖子只有一页\n",
    "        if len(data) == 0:\n",
    "            return\n",
    "        new_url = [i.find('a').get('href') for i in data if i.find('a') is not None]\n",
    "        if len(new_url) == 0:\n",
    "            return\n",
    "        res.append(new_url[0])\n",
    "        getNextReplyPageUrl(new_url[0], res)\n",
    "    except Exception as e:\n",
    "        print(\"get attr error!\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到有异常请求从你的 IP 发出，请 <a href=\"https://accounts.douban.com/passport/login?redir=https%3A%2F%2Fwww.douban.com%2Fgroup%2F634189%2Fdiscussion%3Fstart%3D0\">登录</a> 使用豆瓣。\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-105652acc8e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# 要么把这个输出文件删了重新跑程序， 要么在上面我备注的改爬取页数的地方，修改将要爬取的页，从上次停止的地方继续.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'rencai.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'utf_8_sig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0me_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'第%d页'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'||'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'发帖时间：%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'||'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'耗时：%f秒'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me_time\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import logging\n",
    "HEADERS_LIST = [\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "            \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "        ]\n",
    "\n",
    "count = 1\n",
    "# NOTE: 在这里改你想要爬那几页，range(a, b)表示爬第a页到第(b - 1)页的， 比如下面的range(0, 10)就是爬第0页到第9页(计算机计数一般从0开始记而不是从1)\n",
    "for page in tqdm(range(0, 1)):\n",
    "    \n",
    "    if count % 18 == 0:\n",
    "        time.sleep(random.randint(220, 230))\n",
    "    \n",
    "    if count % 37 == 0:\n",
    "        time.sleep(random.randint(265, 280))\n",
    "    \n",
    "    if count % 56 == 0:\n",
    "        time.sleep(random.randint(303, 336))\n",
    "    \n",
    "    HEADER = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "    \n",
    "    s_time = time.time()\n",
    "    \n",
    "    filtered_data, topic_url, people_url = get_data(page, HEADER)\n",
    "  \n",
    "    #topic_url = [i for i in topic_url if i.split(':')[0] != 'https']\n",
    "    for i, j in enumerate(topic_url):\n",
    "        if j.split(':')[0] != 'https':\n",
    "            topic_url[i] = topic_url[-1]   \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # NOTE: 如果只想爬第一页回复，就把get_content函数的第2个参数设为'1stPage', 如果想爬所有页的回复，把get_content函数的第二个参数设为'allPages'\n",
    "    imgNumsInTopic_, topicDocs_, topicWays_, allWaysOfReplyContents_, allContsOfReplyContents_, allNicknamesOfReplyContents_, allIdsOfReplyContents_, most_popular = get_content(topic_url, '1stPage')\n",
    "    \n",
    "    jingpin = []\n",
    "    for i in filtered_data:\n",
    "        try:\n",
    "            jingpin.append(i[-5])\n",
    "        except:\n",
    "            jingpin.append('')\n",
    "    \n",
    "    try:\n",
    "        titles = [i[-4] for i in filtered_data]\n",
    "    except:\n",
    "        titles = [i[0] for i in filtered_data]\n",
    "    try:\n",
    "        names = [i[-3] for i in filtered_data]\n",
    "        responses_num = [i[-2] for i in filtered_data]\n",
    "        times = [i[-1] for i in filtered_data]\n",
    "\n",
    "        #if len(allWaysOfReplyContents_) != len(titles):\n",
    "         #   allWaysOfReplyContents_.append(allWaysOfReplyContents_[-1])\n",
    "\n",
    "        csv = pd.DataFrame({'title':titles, 'name':names, 'id':people_url,\n",
    "                            'responses_num':responses_num, 'time':times, 'docs':topicDocs_,\n",
    "                            'reply_content':allContsOfReplyContents_, 'img_nums':imgNumsInTopic_, \n",
    "                            'topic_url':topic_url, 'reply_id':allIdsOfReplyContents_, 'jinpin':jingpin, \n",
    "                            'popular_index':most_popular, 'topic_ways':topicWays_, \n",
    "                            'replyWays':allWaysOfReplyContents_})\n",
    "        # NOTE: 1) 这里面输出的csv文件名, e.g. rencai.csv, 要和data_vis.ipynb里读入的一致. \n",
    "        # 2) 写文件的时候是在末尾append, 如果你强制终止了程序，或者程序中间挂了，为了避免某些页重复被爬从而重复统计，\n",
    "        # 要么把这个输出文件删了重新跑程序， 要么在上面我备注的改爬取页数的地方，修改将要爬取的页，从上次停止的地方继续.\n",
    "        csv.to_csv(r'rencai.csv', mode= 'a', header= True, encoding= 'utf_8_sig')\n",
    "        time.sleep(random.randrange(6, 8, 1))\n",
    "        e_time = time.time()\n",
    "        log = '第%d页'%(page+1) + '||' + '发帖时间：%s'%times[-1] + '||' + '耗时：%f秒'%(e_time-s_time)\n",
    "        print(log)\n",
    "        count = count + 1\n",
    "    except IOError:\n",
    "        print(\"I/O error\")\n",
    "    except IndexError:\n",
    "        logging.error(traceback.format_exc())\n",
    "    except Exception as e:\n",
    "        logging.error(traceback.format_exc())\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
