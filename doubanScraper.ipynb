{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.9/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.9/site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (4.61.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#随机更换代理，是反爬的trick之一，但是在实践过程中，并不work；\n",
    "#更换header也是，但是从爬取的实践经验来看，time.sleep()的设置是非常重要的；\n",
    "#sleep的时间设置长些，也就意味着访问url的间隔时间会长些，这样可以有效避免反爬；\n",
    "import re\n",
    "import requests\n",
    "import urllib\n",
    "import sys\n",
    "!{sys.executable} -m pip install bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "import csv\n",
    "from itertools import cycle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "!{sys.executable} -m pip install tqdm\n",
    "import tqdm\n",
    "\n",
    "def get_data(page, HEADER):\n",
    "    #一页有25个帖子\n",
    "# NOTE: 把代码里的链接换成你想爬取的小组    https://www.douban.com/group/gua/discussion 瓜组\n",
    "# https://www.douban.com/group/634189/discussion  人才组\n",
    "    basic_url = 'https://www.douban.com/group/634189/discussion?start=%d'%(page*25)\n",
    "    #basic_url = 'https://www.douban.com/group/638298/discussion?start=0'\n",
    "    # NOTE: 登录你的账号，用浏览器inspect element找到你自己的cookie, 把下面一行的cookies用你自己的cookie代替\n",
    "    response = requests.get(basic_url, headers = HEADER, timeout= 60, proxies=urllib.request.getproxies(), cookies={'dbcl2':'ReplaceWithYourOwn'})\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    #获取帖子中的title,name,response,time;\n",
    "    data = soup.find_all('tr')\n",
    "    #filtered_data[i] = ['精品', 'title','name','response_nums','time'] or ['title','name','response_nums','time']\n",
    "    filtered_data = [[i for i in i.text.strip().split('\\n')if i.strip() != ''] for i in data[1:]]\n",
    "    \n",
    "    #获取帖子中的topic's url及发帖人id；\n",
    "    topic_people_urls = [url.get('href') for url in soup.find_all('a')]\n",
    "    topic_url = []\n",
    "    people_url = []\n",
    "    for topic_people_url in topic_people_urls:\n",
    "        if topic_people_url is not None:\n",
    "            topic_people_url_ = topic_people_url.split('/')\n",
    "            #topic_url in one page;\n",
    "            if len(topic_people_url_) < 3:\n",
    "                continue\n",
    "            if topic_people_url_[-3] == 'topic':\n",
    "                topic_url.append(topic_people_url)\n",
    "            #people_url in one page;\n",
    "            elif topic_people_url_[-3] == 'people':\n",
    "                people_url.append(topic_people_url)         \n",
    "        else:\n",
    "            topic_url.append('')\n",
    "            people_url.append('')\n",
    "            filtered_data.append('')\n",
    "    return filtered_data, topic_url, people_url\n",
    "\n",
    "def get_most_popular_cont(soup):\n",
    "    dataOfMostPopular = soup.find_all('ul', \"topic-reply popular-bd\")[0]\n",
    "\n",
    "    #最赞回应的发帖方式（电脑orApp）；\n",
    "    waysToPostsInMostPopular = []\n",
    "    for i in dataOfMostPopular.find_all('li'):\n",
    "        try:\n",
    "            wayToPost = i.find_all('span', 'via')[0].text\n",
    "        except:\n",
    "            wayToPost = ''\n",
    "        waysToPostsInMostPopular.append(wayToPost)\n",
    "\n",
    "    #最赞回应的发帖内容；\n",
    "    contsOfMostPopular = [i.text for i in dataOfMostPopular.find_all('p')]\n",
    "\n",
    "    #最赞回应用户的nickname；\n",
    "    nickNamesOfMostPopular = [i.text.split('\\n')[1] for i in dataOfMostPopular.find_all('h4')]\n",
    "\n",
    "    #最赞回应用户的id；\n",
    "    idsOfMostPopular = [i.find('a').get('href') for i in dataOfMostPopular.find_all('h4')]\n",
    "    \n",
    "    return waysToPostsInMostPopular, contsOfMostPopular, nickNamesOfMostPopular, idsOfMostPopular\n",
    "\n",
    "\n",
    "def get_way_id_name_cont(soup):\n",
    "    data = soup.find_all('div', 'reply-doc content')\n",
    "    #发帖方式（电脑orApp）\n",
    "    ways = []\n",
    "    for i in data:\n",
    "        try:\n",
    "            way = i.find_all('span', 'via')[0].text\n",
    "        except:\n",
    "            way = ''\n",
    "        ways.append(way)\n",
    "\n",
    "    #回复内容；\n",
    "    conts = [j.text for i in data for j in i.find_all('p')]\n",
    "\n",
    "    #回复人的nickname;\n",
    "    nickNames = [i.find_all('a')[0].text for i in data]\n",
    "    \n",
    "    #回复人的id\n",
    "    ids = [i.find_all('a')[0].get('href') for i in data]\n",
    "    \n",
    "    return ways, conts, nickNames, ids\n",
    "\n",
    "# 提取帖子的收藏数量\n",
    "def get_favourite_count(soup):\n",
    "    data = soup.find_all('div', 'action-collect')\n",
    "    # 收藏\n",
    "    for i in data:\n",
    "        try:\n",
    "            favourite = i.find_all('span', 'react-num')[0].text\n",
    "            if (len(favourite) == 0):\n",
    "                favourite = '0'\n",
    "        except:\n",
    "            favourite = '0'\n",
    "    return favourite\n",
    "\n",
    "# 提取帖子的点赞数量\n",
    "def get_like_count(soup):\n",
    "    data = soup.find_all('div', 'action-react')\n",
    "    \n",
    "    for i in data:\n",
    "        try:\n",
    "            like = i.find_all('span', 'react-num')[0].text\n",
    "            if (len(like) == 0):\n",
    "                like = '0'\n",
    "        except:\n",
    "            like = '0'\n",
    "    return like\n",
    "\n",
    "# 提取帖子的转发数量\n",
    "def get_sharing_count(soup):\n",
    "    data = soup.find_all('div', 'sharing')\n",
    "    \n",
    "    for i in data:\n",
    "        try:\n",
    "            sharing = i.find_all('span', 'rec-num')[0].text\n",
    "            if (len(sharing) == 0):\n",
    "                sharing = '0'\n",
    "        except:\n",
    "            sharing = '0'\n",
    "    return sharing\n",
    "\n",
    "\n",
    "def get_content(people_url, topic_url, custom):\n",
    "    \n",
    "    allWaysOfReplyContents = []\n",
    "    allContsOfReplyContents = []\n",
    "    allNicknamesOfReplyContents = []\n",
    "    allIdsOfReplyContents = []\n",
    "    allWaysOfReplyContents_ = []\n",
    "    allContsOfReplyContents_ = []\n",
    "    allNicknamesOfReplyContents_ = []\n",
    "    allIdsOfReplyContents_ = []\n",
    "    imgNumsInTopic_ = []\n",
    "    topicDocs_ = []\n",
    "    topicWays_ = []\n",
    "    most_popular = []\n",
    "    favouriteCount_ = []\n",
    "    likeCount_ = []\n",
    "    sharingCount_ = []\n",
    "    \n",
    "#     for url in topic_url:\n",
    "    for (url, people) in zip(topic_url, people_url):\n",
    "        if len(people) == 0:\n",
    "            continue\n",
    "        HEADER = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "        response = requests.get(url, headers=HEADER, timeout= 60, proxies=urllib.request.getproxies(), cookies={'dbcl2':'239539523:tY0s4mATKL8'})\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        #帖子中包含图片的数量；\n",
    "        try:\n",
    "            imgNumsInTopic = len(soup.find_all('div', \"rich-content topic-richtext\")[0].find_all('img'))\n",
    "        except:\n",
    "            imgNumsInTopic = 0\n",
    "        imgNumsInTopic_.append(imgNumsInTopic)\n",
    "        \n",
    "        topic = [i for i in soup.find_all('div','topic-doc')[0].text.split('\\n') if (i != '' and len(soup.find_all('div','topic-doc')) > 0)]\n",
    "        \n",
    "        try:\n",
    "            topicDocs = ''.join([i.text for i in soup.find_all('div', \"rich-content topic-richtext\")[0].find_all('p')])\n",
    "        except:\n",
    "            topicDocs = ''\n",
    "        \n",
    "        topicWays = topic[-1] if topicDocs != 'NA' else 0\n",
    "        \n",
    "        topicDocs_.append(topicDocs)\n",
    "        topicWays_.append(topicWays)\n",
    "        \n",
    "        #帖子被收藏，点赞, 转发的次数\n",
    "        favouriteCount = get_favourite_count(soup)\n",
    "        favouriteCount_.append(favouriteCount)\n",
    "        likeCount = get_like_count(soup)\n",
    "        likeCount_.append(likeCount)\n",
    "        sharingCount = get_sharing_count(soup)\n",
    "        sharingCount_.append(sharingCount)\n",
    "        \n",
    "        if custom == 'mostPopular':\n",
    "            #最赞回应的发帖方式（电脑orApp）、内容、昵称及id；\n",
    "            waysToPostsInMostPopular, contsOfMostPopular, nickNamesOfMostPopular, idsOfMostPopular = get_most_popular_cont(soup)\n",
    "            allWaysOfReplyContents_.append(waysToPostsInMostPopular)\n",
    "            allContsOfReplyContents_.append(contsOfMostPopular)\n",
    "            allNicknamesOfReplyContents_.append(nickNamesOfMostPopular)\n",
    "            allIdsOfReplyContents_.append(idsOfMostPopular)\n",
    "        \n",
    "        elif custom == '1stPage':\n",
    "            #第一页的发帖方式（电脑orApp）、内容、昵称及id；\n",
    "            waysOfReplyIn1stPage, contsOfReplyIn1stPage, nickNamesOfReplyIn1stPage, idsOfReplyIn1stPage = get_way_id_name_cont(soup)\n",
    "            try:\n",
    "                len_popular, _, _, _ = get_most_popular_cont(soup)\n",
    "                index = len(len_popular)\n",
    "            except:\n",
    "                index = 'none'\n",
    "            allWaysOfReplyContents_.append(waysOfReplyIn1stPage)\n",
    "            allContsOfReplyContents_.append(contsOfReplyIn1stPage)\n",
    "            allNicknamesOfReplyContents_.append(nickNamesOfReplyIn1stPage)\n",
    "            allIdsOfReplyContents_.append(idsOfReplyIn1stPage)\n",
    "            most_popular.append(index)\n",
    "            \n",
    "        elif custom == 'allPages':\n",
    "            #全部页的发帖方式（电脑orApp）、内容、昵称及id；\n",
    "#             waysOfReplyIn1stPage, contsOfReplyIn1stPage, nickNamesOfReplyIn1stPage, idsOfReplyIn1stPage = get_way_id_name_cont(soup)\n",
    "#             allWaysOfReplyContents.extend(waysOfReplyIn1stPage)\n",
    "#             allContsOfReplyContents.extend(contsOfReplyIn1stPage)\n",
    "#             allNicknamesOfReplyContents.extend(nickNamesOfReplyIn1stPage)\n",
    "#             allIdsOfReplyContents.extend(idsOfReplyIn1stPage)\n",
    "            allPagesOfReplyUrls = [url]\n",
    "            getNextReplyPageUrl(url, allPagesOfReplyUrls)\n",
    "            for page in allPagesOfReplyUrls:\n",
    "                response = requests.get(page, headers = HEADER, timeout= 60, proxies=urllib.request.getproxies(), cookies={'dbcl2':'239539523:tY0s4mATKL8'})\n",
    "                htmlOfPage = response.text\n",
    "                soupOfPage = BeautifulSoup(html, 'lxml')\n",
    "                ways, conts, nickNames, ids = get_way_id_name_cont(soupOfPage)\n",
    "                allWaysOfReplyContents.extend(ways)\n",
    "                allContsOfReplyContents.extend(conts)\n",
    "                allNicknamesOfReplyContents.extend(nickNames)\n",
    "                allIdsOfReplyContents.extend(ids)\n",
    "                time.sleep(random.uniform(3.0, 5.1))\n",
    "            \n",
    "            allWaysOfReplyContents_.append(allWaysOfReplyContents)\n",
    "            allContsOfReplyContents_.append(allContsOfReplyContents)\n",
    "            allNicknamesOfReplyContents_.append(allNicknamesOfReplyContents)\n",
    "            allIdsOfReplyContents_.append(allIdsOfReplyContents)\n",
    "            \n",
    "        time.sleep(random.uniform(3.0, 5.1))\n",
    "                       \n",
    "    return imgNumsInTopic_, topicDocs_, topicWays_, allWaysOfReplyContents_, allContsOfReplyContents_, allNicknamesOfReplyContents_, allIdsOfReplyContents_, most_popular, favouriteCount_, likeCount_, sharingCount_\n",
    "\n",
    "def getNextReplyPageUrl(url, res):\n",
    "    '''\n",
    "\n",
    "    :return:\n",
    "    '''\n",
    "    if url==None or len(url)==0:\n",
    "        return\n",
    "    try:\n",
    "        response = requests.get(url, headers = HEADER, timeout= 60, proxies=urllib.request.getproxies(), cookies={'dbcl2':'239539523:tY0s4mATKL8'})\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        data = soup.findAll('span', 'next')\n",
    "        # 帖子只有一页\n",
    "        if len(data) == 0:\n",
    "            return\n",
    "        new_url = [i.find('a').get('href') for i in data if i.find('a') is not None]\n",
    "        if len(new_url) == 0:\n",
    "            return\n",
    "        res.append(new_url[0])\n",
    "        getNextReplyPageUrl(new_url[0], res)\n",
    "    except Exception as e:\n",
    "        print(\"get attr error!\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import traceback\n",
    "import logging\n",
    "HEADERS_LIST = [\n",
    "            \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "            \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "            \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "            \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "            \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "            \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n",
    "            \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "            \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n",
    "        ]\n",
    "\n",
    "count = 1\n",
    "# NOTE: 在这里改你想要爬那几页，range(a, b)表示爬第a页到第(b - 1)页的， 比如下面的range(0, 10)就是爬第0页到第9页(计算机计数一般从0开始记而不是从1)\n",
    "for page in tqdm(range(0, 1)):\n",
    "    \n",
    "    if count % 18 == 0:\n",
    "        time.sleep(random.randint(220, 230))\n",
    "    \n",
    "    if count % 37 == 0:\n",
    "        time.sleep(random.randint(265, 280))\n",
    "    \n",
    "    if count % 56 == 0:\n",
    "        time.sleep(random.randint(303, 336))\n",
    "    \n",
    "    HEADER = {'User-Agent': random.choice(HEADERS_LIST)}\n",
    "    \n",
    "    s_time = time.time()\n",
    "    \n",
    "    filtered_data, topic_url, people_url = get_data(page, HEADER)\n",
    "    \n",
    "    #topic_url = [i for i in topic_url if i.split(':')[0] != 'https']\n",
    "    for i, j in enumerate(topic_url):\n",
    "        if j.split(':')[0] != 'https':\n",
    "            topic_url[i] = topic_url[-1]   \n",
    "    time.sleep(random.uniform(3.0, 5.1))\n",
    "    \n",
    "    # NOTE: 如果只想爬第一页回复，就把get_content函数的第2个参数设为'1stPage', 如果想爬所有页的回复，把get_content函数的第二个参数设为'allPages'(不建议做这个，非常慢，还可能会被封)\n",
    "    imgNumsInTopic_, topicDocs_, topicWays_, allWaysOfReplyContents_, allContsOfReplyContents_, allNicknamesOfReplyContents_, allIdsOfReplyContents_, most_popular_, favouriteCount_, likeCount_, sharingCount_ = get_content(people_url, topic_url, '1stPage')\n",
    "    \n",
    "    # format of each row in i: ['精华', 'title','name','response_nums','time'] or ['title','name','response_nums','time'] or ['title','name','time']\n",
    "    jingpin = []\n",
    "    for i in filtered_data:\n",
    "        if len(i) >= 5:\n",
    "            jingpin.append(i[-5])  \n",
    "        elif len(i) >= 3:\n",
    "            jingpin.append('')\n",
    "    \n",
    "    '''\n",
    "    The below code skip the following lines in the page for logged on account\n",
    "    ['我的钱包']\n",
    "    ['帐号管理']\n",
    "    ['退出']\n",
    "    ['讨论', '作者回应最后回应']\n",
    "    '''\n",
    "    for i in filtered_data:\n",
    "        try:\n",
    "            titles = i[-4]\n",
    "        except:\n",
    "            print(\"failure path:\", i)\n",
    "            pass\n",
    "\n",
    "#         titles = [i[0] for i in filtered_data if len(i) > 0]\n",
    "#         titles = ['' for i in filtered_data]\n",
    "\n",
    "    # 有回复的帖子每行有如下列: 'title','name','response_nums','time'\n",
    "    # 0回复的帖子每行有如下列: 'title','name','time'\n",
    "    titles = []\n",
    "    names = []\n",
    "    responses_num = []\n",
    "    times = []\n",
    "    try:\n",
    "        for i in filtered_data:\n",
    "            if len(i) == 3:\n",
    "                titles.append(i[-3])\n",
    "                names.append(i[-2])\n",
    "                responses_num.append('0')\n",
    "                times.append(i[-1])\n",
    "            elif len(i) >= 4:\n",
    "                titles.append(i[-4])\n",
    "                names.append(i[-3])\n",
    "                responses_num.append(i[-2])\n",
    "                times.append(i[-1])    \n",
    "        csv = pd.DataFrame({'title':titles, 'name':names, 'id':people_url,\n",
    "                            'responses_num':responses_num, 'time':times, 'docs':topicDocs_,\n",
    "                            'reply_content':allContsOfReplyContents_, 'img_nums':imgNumsInTopic_, \n",
    "                            'topic_url':topic_url, 'reply_id':allIdsOfReplyContents_, 'jinpin':jingpin, \n",
    "                            'popular_index':most_popular_, 'topic_ways':topicWays_, \n",
    "                            'replyWays':allWaysOfReplyContents_, 'favourite_count':favouriteCount_,\n",
    "                            'like_count':likeCount_, 'sharing_ocunt':sharingCount_})\n",
    "        # NOTE: 1) 这里面输出的csv文件名, e.g. rencai.csv, 要和data_vis.ipynb里读入的一致. \n",
    "        # 2) 写文件的时候是在末尾append, 如果你强制终止了程序，或者程序中间挂了，为了避免某些页重复被爬从而重复统计，\n",
    "        # 要么把这个输出文件删了重新跑程序， 要么在上面我备注的改爬取页数的地方，修改将要爬取的页，从上次停止的地方继续.\n",
    "        csv.to_csv(r'rencai.csv', mode= 'a', header= True, encoding= 'utf_8_sig')\n",
    "        time.sleep(random.randrange(6.0, 8.0, 1))\n",
    "        e_time = time.time()\n",
    "        log = '第%d页'%(page+1) + '||' + '发帖时间：%s'%times[-1] + '||' + '耗时：%f秒'%(e_time-s_time)\n",
    "        print(log)\n",
    "    except KeyboardInterrupt:\n",
    "        logging.error(traceback.format_exc())\n",
    "        raise\n",
    "    except ValueError:\n",
    "        logging.error(traceback.format_exc())\n",
    "        print(titles, len(titles)) # 29\n",
    "        print(names, len(names)) # 29\n",
    "        print(responses_num, len(responses_num)) #29\n",
    "        print(times, len(times)) # 29\n",
    "        print(jingpin, len(jingpin)) #29\n",
    "        print(topicDocs_, len(topicDocs_)) #30\n",
    "        print(allContsOfReplyContents_, len(allContsOfReplyContents_)) #30\n",
    "        print(imgNumsInTopic_, len(imgNumsInTopic_)) # 30\n",
    "        print(people_url, len(people_url)) # 30\n",
    "        print(topic_url, len(topic_url)) # 30\n",
    "        print(allIdsOfReplyContents_, len(allIdsOfReplyContents_))#30\n",
    "        print(most_popular_, len(most_popular_))#30\n",
    "        print(topicWays_, len(topicWays_))#30\n",
    "        print(allWaysOfReplyContents_, len(allWaysOfReplyContents_)) #30\n",
    "        print(favouriteCount_, len(favouriteCount_)) #30\n",
    "        \n",
    "    except:\n",
    "        logging.error(traceback.format_exc())\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
